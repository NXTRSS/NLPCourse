{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530e6c43",
      "metadata": {
        "id": "530e6c43"
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "!pip install svgling==0.3.1\n",
        "!pip install gensim==4.1.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8chqilA57nZl"
      },
      "id": "8chqilA57nZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f415be77",
      "metadata": {
        "id": "f415be77"
      },
      "source": [
        "## Corpuses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d6c3cf1",
      "metadata": {
        "id": "2d6c3cf1"
      },
      "outputs": [],
      "source": [
        "import nltk.corpus\n",
        "print(dir(nltk.corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4dcbd2b",
      "metadata": {
        "id": "f4dcbd2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b773af2e",
      "metadata": {
        "id": "b773af2e"
      },
      "outputs": [],
      "source": [
        "# ściągnięcie korpusu brown za pomocą biblioteki nltk\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown as cb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c103c19b",
      "metadata": {
        "id": "c103c19b"
      },
      "outputs": [],
      "source": [
        "#Wypiszmy początek corpusu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "528cc478",
      "metadata": {
        "id": "528cc478"
      },
      "outputs": [],
      "source": [
        "#Wypiszmy początek corpusu podzielonego na słowa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d093a6",
      "metadata": {
        "id": "61d093a6"
      },
      "outputs": [],
      "source": [
        "#Zobaczmy oznaczone słowa w kategorii \"news\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5974cdfa",
      "metadata": {
        "id": "5974cdfa"
      },
      "outputs": [],
      "source": [
        "#Zobaczmy zdania w kategorii 'news', 'editorial', 'reviews'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7a5857",
      "metadata": {
        "id": "0d7a5857"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('webtext')\n",
        "from nltk.corpus import webtext\n",
        "#Sprawdźmy korpus webtext\n",
        "for fieldid in webtext.fileids():\n",
        "  print(fieldid, webtext.raw(fieldid)[:80], '...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e209bc",
      "metadata": {
        "id": "a0e209bc"
      },
      "outputs": [],
      "source": [
        "nltk.download('inaugural')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea19a47",
      "metadata": {
        "id": "fea19a47"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import inaugural\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "#Wyrysujmy wykres występowania słowa 'citizen' i 'america' w przemowach inaguracyjnych prezydentów USA\n",
        "figure(figsize=(12, 6), dpi=80)\n",
        "[]\n",
        "cfd = nltk.ConditionalFreqDist((target, fileid[:4]) for fileid in inaugural.fileids() for w in inaugural.words(fileid) \n",
        "for target in ['america', 'citizen'] if w.lower().startswith(target))\n",
        "cfd.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b472d0e",
      "metadata": {
        "id": "4b472d0e"
      },
      "source": [
        "# Spacy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f4ce73",
      "metadata": {
        "id": "e9f4ce73"
      },
      "outputs": [],
      "source": [
        "import spacy as spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a52ca5",
      "metadata": {
        "id": "67a52ca5"
      },
      "outputs": [],
      "source": [
        "!spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbde633e",
      "metadata": {
        "id": "dbde633e"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598e378e",
      "metadata": {
        "id": "598e378e"
      },
      "outputs": [],
      "source": [
        "text1 = ('''\n",
        "The witcher halted at a distance of ten paces.  His sword, slowly drawn from its black enameled sheath, glistened and glowed above his head.\n",
        "“It’s silver,” he said.  “This blade is silver.”\n",
        "The pale little face did not flinch; the anthracite eyes did not change expression.\n",
        "“You’re so like a rusalka, “the witcher continued calmly, “that you could deceive anyone.  All the more as you’re a rare bird, black-haired one.  But horses are never mistaken.  They recognize creatures like you instinctively and perfectly.  What are you?  I think you’re a moola, or an alpor.  An ordinary vampire couldn’t come out in the sun.”\n",
        "The corners of the pale lips quivered and turned up a little.\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8079e33c",
      "metadata": {
        "id": "8079e33c"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0b6f3f",
      "metadata": {
        "id": "3c0b6f3f"
      },
      "outputs": [],
      "source": [
        "#Proszę wypisać listę rzeczowników i przymiotników powyższego tekstu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1faa5df",
      "metadata": {
        "id": "d1faa5df"
      },
      "outputs": [],
      "source": [
        "text2 = ('''\n",
        "Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a \n",
        "degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the \n",
        "new electric power industry. In 1884 he emigrated to the United States, where he became a naturalized citizen. \n",
        "He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. \n",
        "With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in \n",
        "New York to develop a range of electrical and mechanical devices. His alternating current (AC) induction \n",
        "motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable \n",
        "amount of money and became the cornerstone of the polyphase system which that company eventually marketed.\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7e4e86",
      "metadata": {
        "id": "4f7e4e86"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text2)\n",
        "#Proszę wypisać Entities powyższego tekstu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a9c987",
      "metadata": {
        "id": "29a9c987"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331d4b9e",
      "metadata": {
        "id": "331d4b9e"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0766b53",
      "metadata": {
        "id": "b0766b53"
      },
      "outputs": [],
      "source": [
        "text3 = ('''\n",
        "The huge black eyes narrowed.\n",
        "“Where is he, black-haired one?  You were singing, so you’ve drunk some blood.  You’ve taken the ultimate measure, which means you haven’t managed to enslave his mind.  Am I right?”\n",
        "The black-tressed head nodded slightly, almost imperceptibility, and the corners of the mouth turned up even more.  The tiny little face took on an eerie expression.\n",
        "“No doubt you consider yourself the lady of this castle now?”\n",
        "A nod, this time clearer.\n",
        "“Are you a moola?”\n",
        "A slow shake of the head.  The hiss which reverberated through his bones could only have come from the pale, ghastly, smiling lips, although the witcher didn’t see them move.\n",
        "“Alpor?”\n",
        "Denial.\n",
        "The witcher backed away and clasped the hilt of his sword tighter.  “That means you’re-”\n",
        "The corners of the lips started to turn up higher and higher, the lips flew open…\n",
        "“A bruxa!” The witcher shouted, throwing himself towards the fountain.\n",
        "From behind the pale lips glistened white, spiky fangs.  The vampire jumped up, arched her back like a leopard and screamed.\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121d95b7",
      "metadata": {
        "id": "121d95b7"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import math\n",
        "\n",
        "# funkcja TF, dla jednego dokumentu\n",
        "def tf(word, blob):\n",
        "  pass\n",
        "    \n",
        "\n",
        "#Pomocnicza funkcja zliczająca w ilu dokumentach wystąpiło to słow\n",
        "def n_containing(word, bloblist):\n",
        "    pass\n",
        "\n",
        "# funkcja IDF dla wszystkich dokumentów\n",
        "def idf(word, bloblist): \n",
        "    pass\n",
        "\n",
        "#Funkcja TF-IDF\n",
        "def tfidf(word, blob, bloblist):\n",
        "    pass\n",
        "\n",
        "\n",
        "blob1 = TextBlob(text1.lower())\n",
        "blob2 = TextBlob(text2.lower())\n",
        "blob3 = TextBlob(text3.lower())\n",
        "bloblist = [blob1, blob2, blob3]\n",
        "\n",
        "sample_words = ['rusalka', 'witcher', 'Tesla', 'in', 'vampire', 'the']\n",
        "for sample_word in sample_words:\n",
        "    print(f\"For word '{sample_word}' TF Score for text1: {tf(sample_word, blob1):0.4}, IDF Score: {idf(sample_word, bloblist):0.4}, TF-IDF Score for text1:{tfidf(sample_word, blob1, bloblist):0.4}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c015382e",
      "metadata": {
        "id": "c015382e"
      },
      "outputs": [],
      "source": [
        "sample_word = 'tesla'\n",
        "print(f\"For word '{sample_word}' TF Score for text2: {tf(sample_word, blob2):0.4}, IDF Score: {idf(sample_word, bloblist):0.4}, TF-IDF Score for text2:{tfidf(sample_word, blob2, bloblist):0.4}\")\n",
        "\n",
        "sample_word = 'the'\n",
        "print(f\"For word '{sample_word}' TF Score for text2: {tf(sample_word, blob2):0.4}, IDF Score: {idf(sample_word, bloblist):0.4}, TF-IDF Score for text2:{tfidf(sample_word, blob2, bloblist):0.4}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57287fef",
      "metadata": {
        "id": "57287fef"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "textlist = [text1, text2, text3]\n",
        "\n",
        "#Proszę użyć CountVectorizer na naszych 3 dokumentach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15619fa",
      "metadata": {
        "id": "c15619fa"
      },
      "outputs": [],
      "source": [
        "#Jak wygląda słownik CountVectorizer? Na co wskazują te liczby?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d26a26",
      "metadata": {
        "id": "d2d26a26"
      },
      "outputs": [],
      "source": [
        "print(f\"Index for 'witcher': {count_vect.vocabulary_['witcher']}, index for 'rusalka': {count_vect.vocabulary_['rusalka']}, , index for 'tesla': {count_vect.vocabulary_['tesla']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560ed6d9",
      "metadata": {
        "id": "560ed6d9"
      },
      "outputs": [],
      "source": [
        "print(f\"For first text 'witcher' count: {text_counts[0, 238]}, for 'rusalka': {text_counts[0, 181]}, for 'tesla' {text_counts[0, 209]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab22edc",
      "metadata": {
        "id": "cab22edc"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#Wyliczmy TF-IDF za pomocą wbudowanej funkcji sklearn dla naszych tekstów\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8c121e",
      "metadata": {
        "id": "7f8c121e"
      },
      "outputs": [],
      "source": [
        "print(f\"TF-IDF or first text - 'witcher': {text_tfidf[0, 238]}, 'rusalka': {text_tfidf[0, 181]}, 'tesla' {text_tfidf[0, 209]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b327cc35",
      "metadata": {
        "id": "b327cc35"
      },
      "outputs": [],
      "source": [
        "print(f\"TF-IDF or second text - 'witcher': {text_tfidf[1, 238]}, 'rusalka': {text_tfidf[1, 181]}, 'tesla' {text_tfidf[1, 209]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1b5d69",
      "metadata": {
        "id": "af1b5d69"
      },
      "outputs": [],
      "source": [
        "#A jak wyliczyć po prostu TF dla każdego tekstu?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4720a32e",
      "metadata": {
        "id": "4720a32e"
      },
      "source": [
        "# Tokenizacja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eO35lKqDHiM1",
      "metadata": {
        "id": "eO35lKqDHiM1"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a00497",
      "metadata": {
        "id": "b9a00497"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
        "#Zobaczmy jak zostanie stokenizowanie powyższe zdanie\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a13354",
      "metadata": {
        "id": "07a13354"
      },
      "source": [
        "# Stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "riCDprjCHoS_",
      "metadata": {
        "id": "riCDprjCHoS_"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e5710d",
      "metadata": {
        "id": "89e5710d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#Wypiszmy zbiór stopwords dla języka angielskiego\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442a8b79",
      "metadata": {
        "id": "442a8b79"
      },
      "outputs": [],
      "source": [
        "#Sprawdźmy dla jakich języków występuje corpus nltk stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a2657c",
      "metadata": {
        "id": "43a2657c"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"\"\"I go to the grocery store to get tomatoes and apples\"\"\"\n",
        "  \n",
        "#Usuńmy stopwords z powyższego zdania\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4762dec7",
      "metadata": {
        "id": "4762dec7"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d080af",
      "metadata": {
        "id": "11d080af"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "#Sprawdźmy działanie PorterStemmera na powyższych słowach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c161692",
      "metadata": {
        "id": "2c161692"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentence=\"The witcher halted at a distance of ten paces.  His sword, slowly drawn from its black enameled sheath, glistened and glowed above his head. \"\n",
        "#PorterStemmer dla całego zdania\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7aa6e7",
      "metadata": {
        "id": "bc7aa6e7"
      },
      "outputs": [],
      "source": [
        "lancaster = LancasterStemmer()\n",
        "word_list = [\"friend\", \"friendship\", \"friends\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "#Sprawdźmy powyższe słowa za pomocą LancasterStemmer\n",
        "print(\"{0:20}{1:20}{2:20}\". format(\"Word\", \"Porter Stemmer\", \"Lancaster Stemmer\"))\n",
        "for w in word_list:\n",
        "  print(\"{0:20}{1:20}{2:20}\". format(None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95723288",
      "metadata": {
        "id": "95723288"
      },
      "outputs": [],
      "source": [
        "text = \"studies studying cries cry\"\n",
        "#Użyjmy PorterSemmera dla powyższych słów\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5f4a62",
      "metadata": {
        "id": "0b5f4a62"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JybUFh3cISOE",
      "metadata": {
        "id": "JybUFh3cISOE"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13faa1b4",
      "metadata": {
        "id": "13faa1b4"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "#Sprawdźmy działanie WordNetLemmatizer dla powyższych słów\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0623e2de",
      "metadata": {
        "id": "0623e2de"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "#Proszę wypisać porównanie pomiędzy wynikiem lematyzacji dla powyższych wyrazów, pomijając znaki interpunkcyjne\n",
        "\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
        "for w in tokenize:\n",
        "  print(\"{0:20}{1:20}\".format(None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a017ff7f",
      "metadata": {
        "id": "a017ff7f"
      },
      "outputs": [],
      "source": [
        "print(\"{0:20}{1:35}{2:35}\".format(\"Orginal\",\"Part of Speech tag set to Verb\",\"Part of Speech tag set to Noun\"))\n",
        "#Wypiszmy porównanie tak jak powyżej ale ustawmy paramter lematyzacji na pos='v' albo pos='n')\n",
        "for w in tokenize:\n",
        "  print(\"{0:20}{1:35}{2:35}\".format(None,None,None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpU8JHV8IZWe",
      "metadata": {
        "id": "rpU8JHV8IZWe"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88aff160",
      "metadata": {
        "id": "88aff160"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "#poniżej przygotowany słownik pod Part Of Speech Tags jako argument to lematyzacji\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "lemma_function = WordNetLemmatizer()\n",
        "\n",
        "text = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "#Proszę wykonać lematyzację dla powyższych wyrazów, z uwzględnieniem poprawnych POS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bff2c8",
      "metadata": {
        "id": "d0bff2c8"
      },
      "outputs": [],
      "source": [
        "text=\"The witcher halted at a distance of ten paces.  His sword, slowly drawn from its black enameled sheath, glistened and glowed above his head. \"\n",
        "# tokens = word_tokenize(text)\n",
        "#Tak jak powyżej, dla nowego zdania\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47c77ac",
      "metadata": {
        "id": "b47c77ac"
      },
      "outputs": [],
      "source": [
        "tag_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9e4d0c",
      "metadata": {
        "id": "7a9e4d0c"
      },
      "source": [
        "# Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3117277a",
      "metadata": {
        "id": "3117277a"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074448bf",
      "metadata": {
        "id": "074448bf"
      },
      "outputs": [],
      "source": [
        "re.search(r'k.t', 'test k4t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'tottt test'\n",
        "if re.search(r'^k', text):\n",
        "  print(text + text)\n",
        "else:\n",
        "  print(\"Pattern not found\")"
      ],
      "metadata": {
        "id": "rbihANWMB4Fb"
      },
      "id": "rbihANWMB4Fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c8ae6b",
      "metadata": {
        "id": "d3c8ae6b"
      },
      "outputs": [],
      "source": [
        "re.search(r'kot+', 'kottttttttttt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04cce52a",
      "metadata": {
        "id": "04cce52a"
      },
      "outputs": [],
      "source": [
        "re.search(r'^t', 'kottt test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709e022d",
      "metadata": {
        "id": "709e022d"
      },
      "outputs": [],
      "source": [
        "re.search(r't$', 'bottttest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd02a4ee",
      "metadata": {
        "id": "cd02a4ee"
      },
      "outputs": [],
      "source": [
        "re.search(r'k[oa]t', 'kct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809c731c",
      "metadata": {
        "id": "809c731c"
      },
      "outputs": [],
      "source": [
        "re.search(r'k[a-d]t', 'ket')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57dcf93",
      "metadata": {
        "id": "f57dcf93"
      },
      "outputs": [],
      "source": [
        "re.search(r'k[1-4]t', 'k3.3t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4947fe81",
      "metadata": {
        "id": "4947fe81"
      },
      "outputs": [],
      "source": [
        "re.search(r'.+t\\b', 'bottttest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c40404",
      "metadata": {
        "id": "06c40404"
      },
      "outputs": [],
      "source": [
        "re.search(r'\\d\\d', '20')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477e909e",
      "metadata": {
        "id": "477e909e"
      },
      "outputs": [],
      "source": [
        "re.search(r'k\\st', 'k t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed76e33f",
      "metadata": {
        "id": "ed76e33f"
      },
      "outputs": [],
      "source": [
        "re.search(r'k\\St', 'kot')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f13d6b9",
      "metadata": {
        "id": "4f13d6b9"
      },
      "source": [
        "# Word Terms Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536ce30e",
      "metadata": {
        "id": "536ce30e"
      },
      "outputs": [],
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence = 'This device is used to jam the signal'\n",
        "#Proszę sprawdzić definicję słówka 'jam' za pomocą funkcji lesk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea4de5d",
      "metadata": {
        "id": "eea4de5d"
      },
      "outputs": [],
      "source": [
        "sentence = 'I am stuck in a traffic jam'\n",
        "#Proszę sprawdzić definicję słówka 'jam' za pomocą funkcji lesk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb027d30",
      "metadata": {
        "id": "bb027d30"
      },
      "outputs": [],
      "source": [
        "sentence = 'Water current'\n",
        "#Proszę sprawdzić definicję słówka 'current' za pomocą funkcji lesk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f79567d4",
      "metadata": {
        "id": "f79567d4"
      },
      "outputs": [],
      "source": [
        "sentence = 'The current time is 2 AM'\n",
        "#Proszę sprawdzić definicję słówka 'current' za pomocą funkcji lesk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f81531bb",
      "metadata": {
        "id": "f81531bb"
      },
      "source": [
        "# Parsing Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e680181",
      "metadata": {
        "id": "5e680181"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "#Poniżej zdefiniowana prosta gramatyka\n",
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'an' | 'my'\n",
        "N -> 'elephant' | 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7355619d",
      "metadata": {
        "id": "7355619d"
      },
      "outputs": [],
      "source": [
        "groucho_grammar.productions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83fe87d9",
      "metadata": {
        "id": "83fe87d9"
      },
      "outputs": [],
      "source": [
        "from nltk.grammar import *\n",
        "\n",
        "groucho_grammar.productions(lhs=Nonterminal(\"NP\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1fee2f",
      "metadata": {
        "id": "aa1fee2f"
      },
      "outputs": [],
      "source": [
        "groucho_grammar.productions(rhs=Nonterminal(\"Det\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1d1b99",
      "metadata": {
        "id": "3a1d1b99"
      },
      "outputs": [],
      "source": [
        "pp = groucho_grammar.productions(rhs=Nonterminal(\"Det\"))\n",
        "pp[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1a7aeb",
      "metadata": {
        "id": "6d1a7aeb"
      },
      "outputs": [],
      "source": [
        "pp[0].lhs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4f6820",
      "metadata": {
        "id": "3f4f6820"
      },
      "outputs": [],
      "source": [
        "pp[0].rhs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5593588",
      "metadata": {
        "id": "b5593588"
      },
      "outputs": [],
      "source": [
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "#Wyznaczmy drzewo składniowe dla powyższego zdania zgodnie ze zdefiniowaną gramatyką\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ec7beb",
      "metadata": {
        "id": "03ec7beb"
      },
      "outputs": [],
      "source": [
        "#wyrysujmy te drzewo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a66731",
      "metadata": {
        "id": "b3a66731"
      },
      "outputs": [],
      "source": [
        "#wyrysujmy te drzewo w przystępnej formie graficznej\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc462a2",
      "metadata": {
        "id": "efc462a2"
      },
      "outputs": [],
      "source": [
        "#Czy istnieje inne drzewo? Oznaczające inną interpretacje zdania?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ced82d8",
      "metadata": {
        "id": "6ced82d8"
      },
      "source": [
        "# NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72491c8f",
      "metadata": {
        "id": "72491c8f"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.get_pipe(\"ner\").labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('London is a big city in the United Kingdom the United States.')\n",
        "print(\"\\n-------Example 1 ------\\n\")\n",
        "#Proszę wypisać wpydowane w spacy Entities dla powyższego zdania\n",
        "\n",
        "\n",
        "doc1 = nlp('While in France, Christine Lagarde discussed short-term stimulus efforts in a '\n",
        "           'recent interview on 5:00 P.M. with the Wall Street Journal')\n",
        "print(\"\\n-------Example 2 ------\\n\")\n",
        "#Proszę wypisać wpydowane w spacy Entities dla powyższego zdania\n"
      ],
      "metadata": {
        "id": "o6Uk_lbTD9M7"
      },
      "id": "o6Uk_lbTD9M7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc1:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ],
      "metadata": {
        "id": "AX3wYxMNj-w6"
      },
      "id": "AX3wYxMNj-w6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0fd814",
      "metadata": {
        "id": "ac0fd814"
      },
      "outputs": [],
      "source": [
        "!spacy download pl_core_news_sm\n",
        "import spacy\n",
        "nlp_pl=spacy.load(\"pl_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pl.get_pipe(\"ner\").labels"
      ],
      "metadata": {
        "id": "EtBayWiWI9U9"
      },
      "id": "EtBayWiWI9U9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be969e37",
      "metadata": {
        "id": "be969e37"
      },
      "outputs": [],
      "source": [
        "doc=nlp_pl(\"Tomek pracuje w Tesli\")\n",
        "#A Entities dla języka polskiego?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(\"Wcale nie chcemy zdobywać kosmosu, chcemy tylko rozszerzyć Ziemię do jego granic.\")\n",
        "#A Entities dla języka polskiego?\n"
      ],
      "metadata": {
        "id": "MWblWhV7R0fx"
      },
      "id": "MWblWhV7R0fx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11fbb3a7",
      "metadata": {
        "id": "11fbb3a7"
      },
      "outputs": [],
      "source": [
        "doc1 = nlp_pl(u'Francja leży w Europie')\n",
        "#A Entities dla języka polskiego?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5504e91a",
      "metadata": {
        "id": "5504e91a"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "hRfXDbEfyPz-"
      },
      "id": "hRfXDbEfyPz-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "582aaa16",
      "metadata": {
        "id": "582aaa16"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from nltk.corpus import brown    \n",
        "sentences = brown.sents()\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#Proszę wytrenować model Word2Vec za pomocą korpusu brown, długość embeddingu - 100, wielkość okna - 5, \n",
        "#wziąć pod uwagę słowo jeśli występuje chociaż raz, liczbę epok ustawić na 10\n",
        "#na końcu zapisać model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d0642d",
      "metadata": {
        "id": "c0d0642d"
      },
      "outputs": [],
      "source": [
        "#Jak wygląda embedding dla przykładowego słowa?\n",
        "model.wv['computer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610fac55",
      "metadata": {
        "id": "610fac55"
      },
      "outputs": [],
      "source": [
        "#Jakie jest 10 najbliższych wektorów do słówka 'wine'?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('webtext')\n"
      ],
      "metadata": {
        "id": "8QwiUwgJqQt3"
      },
      "id": "8QwiUwgJqQt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "fPG0aL06rQ6R"
      },
      "id": "fPG0aL06rQ6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae89028",
      "metadata": {
        "id": "eae89028"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import webtext   \n",
        "sentences_web = webtext.sents()\n",
        "\n",
        "#Proszę załadować zapisany model i kontynuować trenowanie dla corpusu webtext, przez 4 epoki\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa95583",
      "metadata": {
        "id": "6aa95583"
      },
      "outputs": [],
      "source": [
        "#Jakie jest 10 najbliższych wektorów do słówka 'wine' teraz?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7ee0a6",
      "metadata": {
        "id": "8c7ee0a6"
      },
      "outputs": [],
      "source": [
        "#Co się stanie gdy się zapytamy o niewystępujące słowo?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0af7246",
      "metadata": {
        "id": "b0af7246"
      },
      "outputs": [],
      "source": [
        "#Proszę znaleść najbliższe wektory dla \"algebry emebddingów\": king - man + woman\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac7a700",
      "metadata": {
        "id": "7ac7a700"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da53453a",
      "metadata": {
        "id": "da53453a"
      },
      "outputs": [],
      "source": [
        "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_vectors.most_similar(w2v_vectors['father'] - w2v_vectors['man'] + w2v_vectors['woman'], topn=10)"
      ],
      "metadata": {
        "id": "mMOUbM31r897"
      },
      "id": "mMOUbM31r897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_vectors.most_similar('wine', topn=10)"
      ],
      "metadata": {
        "id": "qbqGGbSfsRd-"
      },
      "id": "qbqGGbSfsRd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c65f851",
      "metadata": {
        "id": "1c65f851"
      },
      "outputs": [],
      "source": [
        "#Jakie jest 10 najbliższych wektorów do słówka 'dog'?\n",
        "w2v_vectors.most_similar('dog', topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3bd8fa5",
      "metadata": {
        "id": "b3bd8fa5"
      },
      "outputs": [],
      "source": [
        "#Proszę znaleść najbliższe wektory dla \"algebry emebddingów\": death - man + computer\n",
        "w2v_vectors.most_similar(w2v_vectors['death'] - w2v_vectors['man'] + w2v_vectors['computer'], topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_vectors['death'].shape"
      ],
      "metadata": {
        "id": "eqt6zMnV13ph"
      },
      "id": "eqt6zMnV13ph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9ff8e6",
      "metadata": {
        "id": "6f9ff8e6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition  import PCA\n",
        "import numpy as np\n",
        "\n",
        "def to_2d(embeddings):\n",
        "    # To reduce embedding dims without losing much information we use PCA\n",
        "    pca = PCA(n_components=2, whiten=True)\n",
        "    pca.fit(embeddings)\n",
        "    return pca.transform(embeddings)\n",
        "\n",
        "\n",
        "def annotated_scatter(points, names, color='blue'):\n",
        "    x_coords = points[:, 0]\n",
        "    y_coords = points[:, 1]\n",
        "    plt.scatter(x_coords, y_coords, c=color)\n",
        "    for label, x, y in zip(names, x_coords, y_coords):\n",
        "                      plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "    plt.xlim(x_coords.min() - .5, x_coords.max() + .5)\n",
        "    plt.ylim(y_coords.min() - .5, y_coords.max() + .5)\n",
        "\n",
        "    \n",
        "def plot_embeddings(embeddings, names, color='blue', show=True):\n",
        "    X_train = np.array([embeddings[k] for k in names])\n",
        "    embeddings_2d = to_2d(X_train)\n",
        "    \n",
        "    annotated_scatter(embeddings_2d, names, color)\n",
        "    plt.grid()\n",
        "    \n",
        "    if show:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673140fb",
      "metadata": {
        "id": "673140fb"
      },
      "outputs": [],
      "source": [
        "near_dog = [elem[0] for elem in w2v_vectors.most_similar(w2v_vectors['dog'])]\n",
        "near_computer = [elem[0] for elem in w2v_vectors.most_similar(w2v_vectors['computer'])]\n",
        "len(near_computer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5324ec",
      "metadata": {
        "id": "9f5324ec"
      },
      "outputs": [],
      "source": [
        "plot_embeddings(w2v_vectors, \n",
        "                near_dog + near_computer, \n",
        "                color=['red'] * len(near_dog) + ['green'] * len(near_computer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34cc9cff",
      "metadata": {
        "id": "34cc9cff"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "import matplotlib.lines as mlines\n",
        "from matplotlib import cm\n",
        "\n",
        "LinearSubs = namedtuple('LinearSubs', \n",
        "                        ('word_pair', 'name'))\n",
        "\n",
        "def plot_linear_substructures(linear_subs, embeddings):\n",
        "    embeddings_matrix = [embeddings[p] for ls in linear_subs for p in ls.word_pair]\n",
        "    embeddings_matrix = np.array(embeddings_matrix)\n",
        "    pair_names = [p for ls in linear_subs for p in ls.word_pair]\n",
        "    ls_names = [ls.name for ls in linear_subs]\n",
        "    embeddings_2d = to_2d(embeddings_matrix)\n",
        "    annotated_scatter(embeddings_2d, \n",
        "                      pair_names, \n",
        "                      cm.Set1.colors[:len(embeddings_2d)])\n",
        "    \n",
        "    for i in range(0, len(embeddings_2d), 2):\n",
        "        p1 = embeddings_2d[i]\n",
        "        p2 = embeddings_2d[i + 1]\n",
        "        # Center of the linear substructure\n",
        "        center = [(p1[i] + p2[i]) / 2 + .04 for i in range(2)]\n",
        "        \n",
        "        plt.plot(*zip(p1, p2), '--')\n",
        "        plt.annotate(ls_names[i // 2], \n",
        "                     xy=center, \n",
        "                     xytext=(0, 0), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeec3fb5",
      "metadata": {
        "id": "aeec3fb5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plot_linear_substructures([LinearSubs(('man', 'woman'), 'sex'),\n",
        "                           LinearSubs(('king', 'queen'), 'sex'),\n",
        "                           LinearSubs(('mother', 'father'), 'sex')], w2v_vectors)\n",
        "\n",
        "plt.subplot(132)\n",
        "plot_linear_substructures([LinearSubs(('cat', 'feline'), 'family'),\n",
        "                           LinearSubs(('dog', 'canine'), 'family'),\n",
        "                           LinearSubs(('parrot', 'bird'), 'family')], w2v_vectors)\n",
        "\n",
        "plt.subplot(133)\n",
        "plot_linear_substructures([LinearSubs(('samsung', 'mobile'), 'product'),\n",
        "                           LinearSubs(('sony', 'tv'), 'product'),\n",
        "                           LinearSubs(('ikea', 'furniture'), 'product')], w2v_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb0a4cf",
      "metadata": {
        "id": "fdb0a4cf"
      },
      "outputs": [],
      "source": [
        "#Glove 6B\n",
        "\n",
        "!curl -OL http://nlp.stanford.edu/data/glove.6B.zip -o glove.6B.zip\n",
        "#wget\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SKsZZYSAQ7Gt",
      "metadata": {
        "id": "SKsZZYSAQ7Gt"
      },
      "outputs": [],
      "source": [
        "!unzip -o glove.6B.zip\n",
        "# !unzip -o /content/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef235f1",
      "metadata": {
        "id": "bef235f1"
      },
      "outputs": [],
      "source": [
        "glove_embeddings = {}\n",
        "with open('glove.6B.300d.txt') as f:\n",
        "    glove_embeddings = {l.split()[0]: np.array(l.split()[1:]).astype('float') for l in f}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings['computer'].shape"
      ],
      "metadata": {
        "id": "fjxskutN4Bs-"
      },
      "id": "fjxskutN4Bs-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe78d30",
      "metadata": {
        "id": "fbe78d30"
      },
      "outputs": [],
      "source": [
        "def get_closest(x, embeddings, topn=3):\n",
        "    \"\"\"\n",
        "    Get the closest embeddings calculating the euclidean distance\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: np.ndarray\n",
        "      Vector containing an embedding\n",
        "    top_k: int, optional\n",
        "      Get the top k similar embeddings\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Dict containing the top k similar embeddings to the given x\n",
        "    \"\"\"\n",
        "    # Stack all embeddings in a single matrix. Note: the matrix dimention will be\n",
        "    # V x D where V is the vocabulary size and D is the embedding dimension\n",
        "    embedding_matrix = np.array(list(embeddings.values()))\n",
        "    # Using broadcasting compute distance to each embedding in our vocabulary\n",
        "    distances = x - embedding_matrix\n",
        "    # Comoute the magnitude of each distance\n",
        "    distances = np.linalg.norm(distances, axis=1)\n",
        "    # Sort distance and keep the smallest k\n",
        "    min_idx = np.argsort(distances)[:topn]\n",
        "    return [list(embeddings)[i] for i in min_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9506e5fd",
      "metadata": {
        "id": "9506e5fd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plot_linear_substructures([LinearSubs(('man', 'woman'), 'sex'),\n",
        "                           LinearSubs(('king', 'queen'), 'sex'),\n",
        "                           LinearSubs(('mother', 'father'), 'sex')], glove_embeddings)\n",
        "\n",
        "plt.subplot(132)\n",
        "plot_linear_substructures([LinearSubs(('cat', 'feline'), 'family'),\n",
        "                           LinearSubs(('dog', 'canine'), 'family'),\n",
        "                           LinearSubs(('parrot', 'bird'), 'family')], glove_embeddings)\n",
        "\n",
        "plt.subplot(133)\n",
        "plot_linear_substructures([LinearSubs(('samsung', 'mobile'), 'product'),\n",
        "                           LinearSubs(('sony', 'tv'), 'product'),\n",
        "                           LinearSubs(('ikea', 'furniture'), 'product')], glove_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12414929",
      "metadata": {
        "id": "12414929"
      },
      "outputs": [],
      "source": [
        "get_closest(glove_embeddings['Rome'] - glove_embeddings['Italy'] + glove_embeddings['France'], glove_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f06d5e9",
      "metadata": {
        "id": "2f06d5e9"
      },
      "outputs": [],
      "source": [
        "# vectors = {name: gensim.downloader.load(name) for name in list(gensim.downloader.info()['models'].keys())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e61431",
      "metadata": {
        "id": "c1e61431"
      },
      "outputs": [],
      "source": [
        "# word1 = 'king'\n",
        "# word2 = 'man'\n",
        "# word3 = 'woman'\n",
        "# for name, vector in vectors.items():\n",
        "#     try:\n",
        "#         result = vector.most_similar(vector[word1] - vector[word2] + vector[word3])\n",
        "#     except:\n",
        "#         print(f\"Calcuation not possible for {name}\")\n",
        "#         print('-'*60)\n",
        "#     else:\n",
        "#         print(f\"Embedding: {name}\")\n",
        "#         for i in result:\n",
        "#             print(i, end=\"\\n\")\n",
        "#         print('-'*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a18e2b0",
      "metadata": {
        "id": "4a18e2b0"
      },
      "outputs": [],
      "source": [
        "# word1 = 'father'\n",
        "# word2 = 'man'\n",
        "# word3 = 'woman'\n",
        "# for vector, name in zip(vectors, gensim.downloader.info()['models'].keys()):\n",
        "#     try:\n",
        "#         result = vector.most_similar(vector[word1] - vector[word2] + vector[word3])\n",
        "#     except:\n",
        "#         print(f\"Calcuation not possible for {name}\")\n",
        "#         print('-'*60)\n",
        "#     else:\n",
        "#         print(f\"Embedding: {name}\")\n",
        "#         for i in result:\n",
        "#             print(i, end=\"\\n\")\n",
        "#         print('-'*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dc34bd",
      "metadata": {
        "id": "05dc34bd"
      },
      "outputs": [],
      "source": [
        "# word1 = 'death'\n",
        "# word2 = 'man'\n",
        "# word3 = 'computer'\n",
        "# for vector, name in zip(vectors, gensim.downloader.info()['models'].keys()):\n",
        "#     try:\n",
        "#         result = vector.most_similar(vector[word1] - vector[word2] + vector[word3])\n",
        "#     except:\n",
        "#         print(f\"Calcuation not possible for {name}\")\n",
        "#         print('-'*60)\n",
        "#     else:\n",
        "#         print(f\"Embedding: {name}\")\n",
        "#         for i in result:\n",
        "#             print(i, end=\"\\n\")\n",
        "#         print('-'*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_plain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ml_nlp)",
      "language": "python",
      "name": "ml_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}